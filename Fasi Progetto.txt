ğŸ“Œ NOTEBOOK: Logistic Regression.ipynb
â”‚
â”œâ”€â”€ FASE 0: Importazione Librerie âœ”ï¸
â”‚   â””â”€ [Codice] import pandas as pd, numpy as np, matplotlib.pyplot, seaborn...
â”‚
â”œâ”€â”€ FASE 1: Caricamento del dataset pulito df_EDA.csv âœ”ï¸
â”‚   â””â”€ [Codice] df = pd.read_csv("data/df_EDA.csv")
â”‚
â”œâ”€â”€ FASE 2: Encoding delle variabili categoriche (Ordinal Encoding) ğŸŸ¡
â”‚   â””â”€ Motivazione: usare un encoding compatibile con la regressione logistica.
â”‚
â”œâ”€â”€ FASE 3: Separazione X (features) e y (target) ğŸŸ¡
â”‚   â””â”€ [Codice] X = df.drop('loan_status', axis=1); y = df['loan_status']
â”‚
â”œâ”€â”€ FASE 4: Train/Test Split ğŸŸ¡
â”‚   â””â”€ [Codice] train_test_split con stratify=y
â”‚
â”œâ”€â”€ FASE 5: Standardizzazione ğŸŸ¡
â”‚   â””â”€ Solo per Regressione Logistica. Usa StandardScaler in una pipeline.
â”‚
â”œâ”€â”€ FASE 6: Costruzione della Pipeline ğŸŸ¡
â”‚   â””â”€ [Codice] Pipeline con StandardScaler + LogisticRegression
â”‚
â”œâ”€â”€ FASE 7: Addestramento (fit) del modello ğŸŸ¡
â”‚
â”œâ”€â”€ FASE 8: Valutazione â€” Predizione + Matrice Confusione ğŸŸ¡
â”‚   â””â”€ Accuracy, Recall, Precision, Confusion Matrix, ROC Curve
â”‚
â”œâ”€â”€ FASE 9: Inferenza Statistica con statsmodels ğŸŸ¡
â”‚   â””â”€ [Codice] sm.Logit per ottenere p-value e coefficienti
â”‚
â”œâ”€â”€ FASE 10: Conclusione e interpretazione ğŸŸ¡
â”‚   â””â”€ Quali feature influenzano? L'inferenza Ã¨ significativa? Il modello Ã¨ utile?
â”‚
â””â”€â”€ ğŸ“Š Grafici: Confusion Matrix, ROC Curve, Coefficienti (barplot con interpretazione) ğŸŸ¡








ğŸ“Œ NOTEBOOK: Decision Tree.ipynb
â”‚
â”œâ”€â”€ FASE 0: Librerie
â”œâ”€â”€ FASE 1: Caricamento df
â”œâ”€â”€ FASE 2: Ordinal Encoding (come per logistica)
â”œâ”€â”€ FASE 3: X e y
â”œâ”€â”€ FASE 4: Train/Test Split
â”œâ”€â”€ FASE 5: Costruzione modello DecisionTreeClassifier
â”‚   â”œâ”€â”€ ğŸ“Œ Pre-Pruning (max_depth, min_samples_split, ecc.)
â”‚   â””â”€â”€ ğŸ“Œ Cross-Validation
â”œâ”€â”€ FASE 6: Fit + Predict
â”œâ”€â”€ FASE 7: Valutazione con accuracy, confusion matrix, ROC
â”œâ”€â”€ FASE 8: Feature Importance (ğŸ“ˆ Gini Importance)
â”œâ”€â”€ FASE 9: Post-Pruning (GridSearchCV o Cost-Complexity Pruning)
â”œâ”€â”€ FASE 10: Conclusione
â””â”€â”€ ğŸ“Š Grafici: Albero, Matrice, Feature Importance, ROC








ğŸ“Œ NOTEBOOK: Random Forest.ipynb
â”œâ”€â”€ Stesse fasi iniziali
â”œâ”€â”€ Costruzione modello con RandomForestClassifier
â”œâ”€â”€ Fit + Predict
â”œâ”€â”€ Valutazione
â”œâ”€â”€ Feature Importance (piÃ¹ robusta)
â”œâ”€â”€ Confronto con Decision Tree
â””â”€â”€ ğŸ“Š ROC, matrice confusione, feature importance







ğŸ“Œ NOTEBOOK: Neural Network.ipynb
â”œâ”€â”€ Librerie (tensorflow.keras)
â”œâ”€â”€ Dataset
â”œâ”€â”€ Encoding
â”œâ”€â”€ Standardizzazione
â”œâ”€â”€ Separazione X/y
â”œâ”€â”€ Costruzione modello sequenziale:
â”‚   â”œâ”€â”€ Dense
â”‚   â”œâ”€â”€ Activation Function (ReLU, Sigmoid)
â”‚   â”œâ”€â”€ Loss Function (categorical crossentropy o binary crossentropy)
â”‚   â”œâ”€â”€ Ottimizzatore (Adam con learning_rate)
â”œâ”€â”€ Compilazione e Fit (epochs, batch_size)
â”œâ”€â”€ Valutazione Accuracy
â”œâ”€â”€ Confusion Matrix
â”œâ”€â”€ Grafico della Loss e Accuracy durante gli Epoch
â”œâ”€â”€ Funzione di attivazione e costo â€” spiegazione
â”œâ”€â”€ Backpropagation (concetto)
â””â”€â”€ Conclusione