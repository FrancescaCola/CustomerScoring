ðŸ“Œ NOTEBOOK: Logistic Regression.ipynb
â”‚
â”œâ”€â”€ FASE 0: Importazione Librerie âœ”ï¸
â”‚   â””â”€ [Codice] import pandas as pd, numpy as np, matplotlib.pyplot, seaborn...
â”‚
â”œâ”€â”€ FASE 1: Caricamento del dataset pulito df_EDA.csv âœ”ï¸
â”‚   â””â”€ [Codice] df = pd.read_csv("data/df_EDA.csv")
â”‚
â”œâ”€â”€ FASE 2: Ordinal Encoding della variabile categorica ('person_education') ðŸŸ¡
|           One-Hot Encoding delle restanti variabili categoriche.
â”‚   â””â”€ Motivazione: usare un encoding compatibile con la regressione logistica.
â”‚
â”œâ”€â”€ FASE 3: Separazione X (features) e y (target) ðŸŸ¡
â”‚   â””â”€ [Codice] X = df.drop('loan_status', axis=1); y = df['loan_status']
â”‚
â”œâ”€â”€ FASE 4: Train/Test Split ðŸŸ¡
â”‚   â””â”€ [Codice] train_test_split con stratify=y
â”‚
â”œâ”€â”€ FASE 5: Standardizzazione ðŸŸ¡
â”‚   â””â”€ Per Regressione Logistica. Usa StandardScaler in una pipeline.
â”‚
â”œâ”€â”€ FASE 6: Costruzione della Pipeline ðŸŸ¡
â”‚   â””â”€ [Codice] Pipeline con StandardScaler + LogisticRegression
    |_ Calcolo VIF (Variance Inflation Factor) per eseguire il funzionamento dell'Inferenza (rimozione > 10)
â”‚
â”œâ”€â”€ FASE 7: Addestramento (fit) del modello ðŸŸ¡
â”‚
â”œâ”€â”€ FASE 8: Valutazione â€” Predizione + Matrice Confusione ðŸŸ¡
â”‚   â””â”€ Accuracy, Recall, Precision, Confusion Matrix, ROC Curve
â”‚
â”œâ”€â”€ FASE 9: Inferenza Statistica con statsmodels ðŸŸ¡
â”‚   â””â”€ [Codice] sm.Logit per ottenere p-value e coefficienti
â”‚
â”œâ”€â”€ FASE 10: Conclusione e interpretazione ðŸŸ¡
â”‚   â””â”€ Quali feature influenzano? L'inferenza Ã¨ significativa? Il modello Ã¨ utile?
â”‚
â””â”€â”€ ðŸ“Š Grafici: Confusion Matrix, ROC Curve, Coefficienti (barplot con interpretazione) ðŸŸ¡








ðŸ“Œ NOTEBOOK: Decision Tree.ipynb
â”‚
â”œâ”€â”€ FASE 0: Librerie
â”œâ”€â”€ FASE 1: Caricamento df
â”œâ”€â”€ FASE 2: Ordinal Encoding (come per logistica)
â”œâ”€â”€ FASE 3: X e y
â”œâ”€â”€ FASE 4: Train/Test Split
â”œâ”€â”€ FASE 5: Costruzione modello DecisionTreeClassifier
â”‚   â”œâ”€â”€ ðŸ“Œ Pre-Pruning (max_depth, min_samples_split, ecc.)
â”‚   â””â”€â”€ ðŸ“Œ Cross-Validation
â”œâ”€â”€ FASE 6: Fit + Predict
â”œâ”€â”€ FASE 7: Valutazione con accuracy, confusion matrix, ROC
â”œâ”€â”€ FASE 8: Feature Importance (ðŸ“ˆ Gini Importance)
â”œâ”€â”€ FASE 9: Post-Pruning (GridSearchCV o Cost-Complexity Pruning)
â”œâ”€â”€ FASE 10: Conclusione
â””â”€â”€ ðŸ“Š Grafici: Albero, Matrice, Feature Importance, ROC








ðŸ“ŒNOTEBOOK: RandomForest 
â”‚
â”œâ”€â”€ 1. Importazione Librerie
â”‚
â”œâ”€â”€ 2. Preparazione Dataset
â”‚   â”œâ”€â”€ a. Scelta e codifica variabili categoriche con Ordinal Encoding
â”‚   â”œâ”€â”€ b. Definizione X (features) e y (target)
â”‚
â”œâ”€â”€ 3. Train-Test Split (stratify = y, 70-30)
â”‚
â”œâ”€â”€ 4. Costruzione Pipeline
â”‚   â”œâ”€â”€ a. ColumnTransformer con OrdinalEncoder
â”‚   â””â”€â”€ b. RandomForestClassifier con class_weight='balanced'
â”‚
â”œâ”€â”€ 5. Addestramento modello
â”‚
â”œâ”€â”€ 6. Valutazione modello
â”‚   â”œâ”€â”€ Accuracy
â”‚   â”œâ”€â”€ Precision
â”‚   â”œâ”€â”€ Recall
â”‚   â”œâ”€â”€ F1-Score
â”‚   â”œâ”€â”€ Confusion Matrix + Heatmap
â”‚   â”œâ”€â”€ ROC Curve + AUC
â”‚
â”œâ”€â”€ 7. Importanza delle feature
â”‚   â””â”€â”€ Classifica variabili piÃ¹ importanti (grafico + interpretazione)
â”‚
â””â”€â”€ 8. Conclusione
    â”œâ”€â”€ Confronto finale con altri modelli
    â””â”€â”€ Interpretazione dei risultati per decidere se Ã¨ il miglior modello







ðŸ“Œ NOTEBOOK: Neural Network.ipynb
â”œâ”€â”€ Librerie (tensorflow.keras)
â”œâ”€â”€ Dataset
â”œâ”€â”€ Encoding
â”œâ”€â”€ Standardizzazione
â”œâ”€â”€ Separazione X/y
â”œâ”€â”€ Costruzione modello sequenziale:
â”‚   â”œâ”€â”€ Dense
â”‚   â”œâ”€â”€ Activation Function (ReLU, Sigmoid)
â”‚   â”œâ”€â”€ Loss Function (categorical crossentropy o binary crossentropy)
â”‚   â”œâ”€â”€ Ottimizzatore (Adam con learning_rate)
â”œâ”€â”€ Compilazione e Fit (epochs, batch_size)
â”œâ”€â”€ Valutazione Accuracy
â”œâ”€â”€ Confusion Matrix
â”œâ”€â”€ Grafico della Loss e Accuracy durante gli Epoch
â”œâ”€â”€ Funzione di attivazione e costo â€” spiegazione
â”œâ”€â”€ Backpropagation (concetto)
â””â”€â”€ Conclusione